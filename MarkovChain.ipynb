{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒãƒ«ã‚³ãƒ•é€£é–\n",
    "äººå·¥ç„¡èƒ½ã®ä¸€ç•ªç°¡å˜ãªä¾‹.  \n",
    "å¤§è¦æ¨¡ãªãƒ„ã‚¤ãƒ¼ãƒˆæƒ…å ±ã‚’ãã‚ŒãŸå‹äººã«æ„Ÿè¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import os,re,random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\r\n",
      "å¯¿å¸ã®ãƒ•ãƒ¬ãƒ³ã‚ºã¨å¯¿å¸ä¼š\r\n",
      "ä»Šå›ã¯å«ã‚‚ä¸€ç·’ã«( â—œÏ‰â€¾)\r\n",
      "ã™ã—ã¾ã¿ã‚Œ æ–°å®¿ã‚»ãƒ³ãƒˆãƒ©ãƒ«ãƒ­ãƒ¼ãƒ‰åº— \r\n",
      "ã‚ã¾ã‚Šã«ã‚‚å‹‰å¼·ã—ãªã•éãã¦é–‹ãã£ã±ãªã—ã®å•é¡Œé›†ã«ãƒ›ã‚³ãƒªãŒä¹—ã£ã¦ã‚‹ã®è‰\r\n",
      "ã€Œä¼‘ã¿ã ã‹ã‚‰å®¶ã«ãªã£ã¤ã‚“ãŒã„ã‚‹å¬‰ã—ã„ã€œï¼ã€ã£ã¦é¨’ã„ã§ãŸã‚‰ã‚«ãƒ¼ãƒšãƒƒãƒˆã«ãŠå¼å½“ä¸¸ã”ã¨ã²ã£ãã‚Šè¿”ã—ãŸ......\r\n",
      " ã‚ã‚‹ã“ã•ã‚“ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™( â—œÏ‰â€¾)\r\n",
      "ã ã£ã¦ã‚„ã£ã¦ã‚‰ã‚“ãªã„ã˜ã‚ƒã‚“\r\n",
      "ã‚¹ãƒˆãƒ¬ã‚¹ã‚ˆã‚Šãƒ­ãƒãƒ³ã‚¹ã§ã—ã‚‡\r\n",
      " ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™( â—œÏ‰â€¾)\r\n"
     ]
    }
   ],
   "source": [
    "!head TWEETandURL.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  ä»Šå›ã¯å«ã‚‚ä¸€ç·’ã«( â—œÏ‰â€¾)<EOS>\n"
     ]
    }
   ],
   "source": [
    "filename=\"TWEETandURL.csv\" #ä»»æ„ã®ãƒ•ã‚¡ã‚¤ãƒ«å\n",
    "nrt_tweet = pd.read_csv(\"TWEETandURL.csv\",encoding='utf-8')[\"text\"][1:]\n",
    "words = []\n",
    "for _ in nrt_tweet:\n",
    "    words.append(_.strip()+\"<EOS>\")\n",
    "print(\"test: \",words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1228\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "tmp=[]\n",
    "#mt = MeCab.Tagger(\"-Ochasen\")\n",
    "mt = MeCab.Tagger(' -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "mt.parse(\"\") #ãŠã¾ã˜ãªã„\n",
    "for i,tweet in enumerate(nrt_tweet[:100]):\n",
    "    #print(tweet,type(tweet))\n",
    "    node = mt.parseToNode(tweet.strip())\n",
    "    while node:\n",
    "        ch = node.surface\n",
    "        if ch == \"\":\n",
    "            node = node.next\n",
    "            continue\n",
    "        tmp.append(ch)\n",
    "        node = node.next\n",
    "print(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ãƒãƒ«ã‚³ãƒ•é€£é–ã®è¾æ›¸\n",
    "def make_dic(words,dic):\n",
    "    tmp = [\"START\"]\n",
    "    for word in words:\n",
    "        #è©²å½“æ–‡å­—ãŒæ¥ãŸã‚‰æ¬¡ã®æ–‡å­—ãƒ˜ã‚¹ã‚­ãƒƒãƒ—\n",
    "        if word==\"\" or word==\"\\n\\t\" or word==\"\\n\":\n",
    "            continue\n",
    "        #3wordsã«ãªã‚‹ã¾ã§æ–‡å­—ã‚’åŠ ãˆã‚‹\n",
    "        tmp.append(word)\n",
    "        if len(tmp) < 3: \n",
    "            continue\n",
    "        #4å˜èª tmpã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹ã®ã§ä¸€ç•ªå¤ã„ä¸€å˜èªã‚’é™¤å»\n",
    "        if len(tmp) > 3:\n",
    "            tmp = tmp[1:]\n",
    "        set_word3(dic,tmp)\n",
    "        if word == \"EOS\":\n",
    "            tmp = [\"START\"]\n",
    "            return dic\n",
    "            #continue\n",
    "    #print(dic)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_word3(dic,s3):\n",
    "    w1,w2,w3 = s3\n",
    "    if not w1 in dic:\n",
    "        dic[w1] = {}\n",
    "    if not w2 in dic[w1]:\n",
    "        dic[w1][w2] = {}\n",
    "    if not w3 in dic[w1][w2]:\n",
    "        dic[w1][w2][w3] = 0\n",
    "    dic[w1][w2][w3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D===============\n",
      "{}\n",
      "\n",
      "D================\n",
      "{'START': {'ãŠè…¹': {'ãŒ': 1}}, 'ãŠè…¹': {'ãŒ': {'ä»Š': 1}}, 'ãŒ': {'ä»Š': {'ã¨ã¦ã‚‚': 1}}, 'ä»Š': {'ã¨ã¦ã‚‚': {'ç©ºã„ãŸ': 1}}, 'ã¨ã¦ã‚‚': {'ç©ºã„ãŸ': {'åŠ©ã‘ã¦ãã‚Œ': 1}}, 'ç©ºã„ãŸ': {'åŠ©ã‘ã¦ãã‚Œ': {'EOS': 1}}}\n",
      "\n",
      "D================\n",
      "{'START': {'ãŠè…¹': {'ãŒ': 1}, 'ä»Š': {'ãƒãƒ³ãƒãƒ¼ã‚¬ãƒ¼': 1}}, 'ãŠè…¹': {'ãŒ': {'ä»Š': 1}}, 'ãŒ': {'ä»Š': {'ã¨ã¦ã‚‚': 1, 'é£Ÿã¹ãŸã„': 1}}, 'ä»Š': {'ã¨ã¦ã‚‚': {'ç©ºã„ãŸ': 1}, 'ãƒãƒ³ãƒãƒ¼ã‚¬ãƒ¼': {'ãŒ': 1}, 'é£Ÿã¹ãŸã„': {'EOS': 1}}, 'ã¨ã¦ã‚‚': {'ç©ºã„ãŸ': {'åŠ©ã‘ã¦ãã‚Œ': 1}}, 'ç©ºã„ãŸ': {'åŠ©ã‘ã¦ãã‚Œ': {'EOS': 1}}, 'ãƒãƒ³ãƒãƒ¼ã‚¬ãƒ¼': {'ãŒ': {'ä»Š': 1}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#æŒ™å‹•ãƒ†ã‚¹ãƒˆ\n",
    "test_dic={}\n",
    "print(\"D===============\\n{}\\n\".format(test_dic))\n",
    "test_dic = make_dic([\"ãŠè…¹\",\"ãŒ\",\"ä»Š\",\"ã¨ã¦ã‚‚\",\"ç©ºã„ãŸ\",\"åŠ©ã‘ã¦ãã‚Œ\",\"EOS\"],test_dic)\n",
    "print(\"D================\\n{}\\n\".format(test_dic))\n",
    "test_dic = make_dic([\"ä»Š\",\"ãƒãƒ³ãƒãƒ¼ã‚¬ãƒ¼\",\"ãŒ\",\"ä»Š\",\"é£Ÿã¹ãŸã„\",\"EOS\"],test_dic)\n",
    "print(\"D================\\n{}\\n\".format(test_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ãƒ„ã‚¤ãƒ¼ãƒˆã®è‡ªå‹•ç”Ÿæˆ\n",
    "def make_tweet(dic,top_word=\"START\"):\n",
    "    ret = []\n",
    "    if not top_word in dic:\n",
    "        return \"no dic\"\n",
    "    #top = dic[\"START\"]\n",
    "    top = dic[top_word]\n",
    "    w1 = word_choice(top)\n",
    "    if top_word != \"START\":\n",
    "        w1 = top_word\n",
    "    w2 = word_choice(top[w1])\n",
    "    ret.append(w1)\n",
    "    ret.append(w2)\n",
    "    flag = True\n",
    "    if w1 == \"EOS\" or w2 == \"EOS\":\n",
    "        flag = False\n",
    "    while flag:\n",
    "        #print(dic[w1][w2])\n",
    "        try:\n",
    "            #print(\"CHECK1: \",w1,w2)\n",
    "            #print(\"CHECK2: \",dic[w1][w2])\n",
    "            w3 = word_choice(dic[w1][w2])\n",
    "            if w3 == \"EOS\":\n",
    "                break\n",
    "            ret.append(w3)\n",
    "            w1,w2 = w2,w3\n",
    "        except {KeyError,TypeError} as e:\n",
    "            print(e)\n",
    "            #print(dic[w1][w2])\n",
    "    return \"\".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_choice(sel):\n",
    "    keys = sel.keys()\n",
    "    L = random.choice(list(keys))\n",
    "    #print(keys,L)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(tweets,markov_file=None):\n",
    "    '''\n",
    "    tweets : å…¨ãƒ„ã‚¤ãƒ¼ãƒˆ\n",
    "    markov_file : è¾æ›¸ã§ã™ã­\n",
    "    '''\n",
    "    #dict_file = markov_file\n",
    "    #if os.path.exists(dict_file):\n",
    "    #    return json.load(open(dict_file,\"r\"))\n",
    "    tagger = MeCab.Tagger('-Owakati')\n",
    "    #tagger = MeCab.Tagger(' -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    dic={}\n",
    "    for i,tweet in enumerate(tweets):\n",
    "        #print(i)\n",
    "        nodes = tagger.parseToNode(tweet)\n",
    "        #è¾æ›¸ã‚’ä½œæˆã™ã‚‹\n",
    "        #dic = make_dic(nodes)\n",
    "        words=[\"\"] #åˆ†ã‹ã¡æ›¸ãã—ãŸå˜èªã‚’ãƒªã‚¹ãƒˆã«åŠ ãˆã‚‹\n",
    "        while nodes:\n",
    "            if nodes.surface==\"\":\n",
    "                nodes = nodes.next\n",
    "            else:\n",
    "                words.append(nodes.surface)\n",
    "                nodes = nodes.next\n",
    "        words.append(\"EOS\")\n",
    "        dic = make_dic(words,dic) #ãƒãƒ«ã‚³ãƒ•é€£é–ã®è¾æ›¸ã¸\n",
    "        #json.dump(dic,open(dict_file,\"w\"))\n",
    "    return dic\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ›¸ãè¾¼ã¿çµ‚äº†\n"
     ]
    }
   ],
   "source": [
    "#è¾æ›¸æ ¼ç´ and ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€ã¨ãç”¨\n",
    "dic = main(nrt_tweet)\n",
    "json.dump(dic,open(\"markov_dict.json\",\"w\"))\n",
    "print(\"æ›¸ãè¾¼ã¿çµ‚äº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,ç°ã«ãªã£ã¡ã‚ƒã£ãŸã‚Šã—ãªã„ç³»ãƒ„ã‚¤ãƒ¼ãƒˆã‹ã¨è©°ã‚è¾¼ã‚“ã ã‚ˆã†ã ï¼ğŸ˜„âœ´âœ¨ã€\n",
      "2,è²·ãŠã†ã¨ãŠã‚‚ã„ã¾ãµãã¿ãµãã¿ã¾ãƒ¼â™ªã—ã¾ã—ãŸã¯ç¾å®Ÿä¸–ç•Œã¯è¨€è‘‰ãŒå‡ºãªã„å¦¥å½“æ€§ã‚’ç¤ºå”†\n",
      "3,ãƒ‡ãƒ¬ã¦\n",
      "4,ã‚±ãƒˆãƒ«ä»¥å¤–é«˜éãã¦ä¿ºã¨LINEã™ã‚‹äººï¼Œãƒãƒ³ãƒãƒ³ç•™å¹´ã—ãŸå¤±æ•—ã—ã¾ã—ãŸğŸ˜ŒğŸ™ŒğŸ»\n",
      "5,ä¸­é«˜ä½“æ“éƒ¨ã£ã¦ã‚ªã‚¿ã‚¯ãŠã™ã™ã‚ã®ç‰›è§’æ–°å®¿æ­Œèˆä¼ç”ºã§é»’äººã®ã‚´ãƒ„ã‚¤ã«ãƒ¼ãŒãƒ¼ï¼¼ï¼¼\\â””('Ï‰')â”˜//ï¼ï¼!!\n",
      "6,....oh........nothankyouï¼ãªã‚“ã‹é«ªã®æ¯›ã‚‚ã˜ã‚ƒã‚‚ã˜ã‚ƒã§ï½µï¾ƒï¾ï½ºã¿ãŸã„ãªãƒ¢ãƒ–ã„ãŸæ®ãˆç½®ãã®ãƒ‘ã‚½ã‚³ãƒ³ãŒå‹æ‰‹ã«è¼‰ã‚‹ã®ãª(ï½¥ã¸ï½¥)\n",
      "7,è©±ã—ç›¸æ‰‹ãŒå±…ãŸã®ã§ãƒ›ã‚¤ãƒ›ã‚¤ä»˜ã„ã¦ã„ã©ã­ã‡......(ï¾ï¾—ï¾ï¾—ï¾ï¾—ï¾ï¾—ï¾ï¾—)\n",
      "8,ã¬ã‚‹ã¬ã‚‹å‹•ãã‚ãã‚„ã¾ã•ã‚“\n",
      "9,ãƒ€ãƒƒã‚¯ã‚¤ãƒ³\n",
      "10,å¹…åºƒã„ã‚¸ãƒ£ãƒ³ãƒ«ã§æ´»ç”¨ã§ãã‚‹ã£ã¦ã€åˆ‡ã‚Šã©ã“ã‚ãã“ã˜ã‚ƒãªã„æœ¬ã¯ç ”ç©¶å®¤é¢æ¥å¯¾ç­–\n"
     ]
    }
   ],
   "source": [
    "#f=open(\"NRT_Markov_Result.csv\",\"w\")\n",
    "for i in range(10):\n",
    "    print(i+1,make_tweet(dic).replace(\"EOS\",\"\"),sep=\",\")\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
