{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# はじめに\n",
    "業務でword2vecを用いる必要性が出てきたため,まずword2vecで実際に遊んでみることにします.  \n",
    "遊んでみたあとには理論を勉強して派生であるdoc2vec(paragraph2vec)も触れたいと思います."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vecはpythonのgensim: https://radimrehurek.com/gensim/models/word2vec.html ライブラリを用いる. また本実験では基本的にpython3系を利用する.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "すでに学習済みモデルを用いている場合は実験へ  \n",
    "# 前処理\n",
    "wikipediaのデータを手に入れる\n",
    "```zsh\n",
    "% curl https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2 -o jawiki-latest-pages-articles.xml.bz2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手に入れたデータがテキストでないのでテキストに変換する.  \n",
    "変換ソフトとしてWikiExtractor.pyをgithubから落としてきて利用する.  \n",
    "```zsh\n",
    "% git clone https://github.com/attardi/wikiextractor\n",
    "% python3 wikiextractor/WikiExtractor.py jawiki-latest-pages-articles.xml.bz2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記プログラムを動かすとtext/以下に複数ファイルが生成されているので一つのファイルにまとめる\n",
    "```zsh\n",
    "% cat text/*/* > jawiki.txt\n",
    "```\n",
    "\n",
    "word2vecに読み込ませるために単語単位にセグメンテーションをする必要がある.  \n",
    "今回はMeCab及びmecab-ipadic-NEologdの辞書を用いて行った.  \n",
    "```zsh \n",
    "% mecab -Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd jawiki.txt -p ddata.txt\n",
    "```\n",
    "これで学習データの準備は整った"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< doc id =\" 5 \" url =\" https :// ja . wikipedia . org / wiki ? curid = 5 \" title =\" アンパサンド \"> \r\n",
      "アンパサンド \r\n",
      "\r\n",
      "アンパサンド (, &) と は 「 … と … 」 を 意味 する 記号 で ある 。 ラテン語 の の 合字 で 、 Trebuchet MS フォント で は 、 と 表示 さ れ \" et \" の 合字 で ある こと が 容易 に わかる 。 ampersa 、 すなわち \" and per se and \"、 その 意味 は \" and [ the symbol which ] by itself [ is ] and \" で ある 。 \r\n",
      "\r\n",
      "その 使用 は 1世紀 に 遡る こと が でき ( 1 )、 5世紀 中葉 ( 2 , 3 ) から 現代 ( 4 - 6 ) に 至る まで の 変遷 が わかる 。 \r\n",
      "Z に 続く ラテン文字 アルファベット の 27 字 目 と さ れ た 時期 も ある 。 \r\n",
      "\r\n",
      "アンパサンド と 同じ 役割 を 果たす 文字 に 「 の et 」 と 呼ば れる 、 数字 の 「 7 」 に 似 た 記号 が あっ た (, U + 204 A )。 この 記号 は 現在 も ゲール文字 で 使わ れ て いる 。 \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head ddata.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec で遊ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec,Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習方法(今回は省略した)\n",
    "```python\n",
    "data = word2vec.Text8Corpus(\"ddata.txt\")\n",
    "model = word2vec.Word2Vec(data,size=200) #sizeは次元数\n",
    "model.save(\"w2v_wikipedia\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"MODEL/w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自前の自然言語処理ライブラリ\n",
    "from load_datasets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "その入力単語(列)と類似した単語候補を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: 佐倉綾音\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('茅野愛衣', 0.8792809844017029),\n",
       " ('真田アサミ', 0.8551074266433716),\n",
       " ('日笠陽子', 0.8525534272193909),\n",
       " ('伊瀬茉莉也', 0.8524771928787231),\n",
       " ('東山奈央', 0.8511958122253418),\n",
       " ('名塚佳織', 0.84943687915802),\n",
       " ('内田真礼', 0.8488936424255371),\n",
       " ('下野紘', 0.8487714529037476),\n",
       " ('福山潤', 0.8474154472351074),\n",
       " ('三森すずこ', 0.8453586101531982)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = split_word(input(\"input text: \"),True).strip().split(\" \")\n",
    "model.most_similar(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"イスラム教\" + \"旧約聖書\" - \"コーラン\" == ??? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ヒンドゥー教', 0.6836835145950317),\n",
       " ('ユダヤ教', 0.6696462035179138),\n",
       " ('キリスト教', 0.6457203030586243),\n",
       " ('キリスト教徒', 0.6282202005386353),\n",
       " ('イスラーム', 0.6207655668258667),\n",
       " ('ムスリム', 0.6194979548454285),\n",
       " ('イスラム教徒', 0.6105567216873169),\n",
       " ('ゾロアスター教', 0.6098810434341431),\n",
       " ('シーア派', 0.6076986193656921),\n",
       " ('正教会', 0.594813346862793)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"イスラム教\", \"旧約聖書\"], negative=[\"コーラン\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ついでにfasttextでも遊ぶ. \n",
    "fasttextの導入及び学習方法について\n",
    "```zsh\n",
    "% git clone https://github.com/facebookresearch/fastText.git\n",
    "% cd fastText\n",
    "% make\n",
    "% ./fasttext skipgram -input ../ddata.txt -output fmodel -dim 200 \n",
    "```\n",
    "\n",
    "せっかくなので学習済みモデルを使用して遊ぶことにした.  \n",
    "学習済みモデルは以下のサイトから拝借しました.\n",
    "- [Qita- fastTextの学習済みモデルを公開しました](http://qiita.com/Hironsan/items/513b9f93752ecee9e670):wikipedia\n",
    "- [pixivnovel2vec](https://github.com/pixiv/pixivnovel2vec/releases):pixiv小説"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers.fasttext import FastText\n",
    "import gensim\n",
    "fmodel = gensim.models.KeyedVectors.load_word2vec_format('MODEL/model.vec', binary=False)\n",
    "#http://marmarossa.hatenablog.com/entry/2017/02/24/021331\n",
    "#fmodel = FastText.load_fasttext_format('MODEL/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: 佐倉綾音\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('種田梨沙', 0.6734929084777832),\n",
       " ('花江夏樹', 0.6484917402267456),\n",
       " ('大亀あすか', 0.6433695554733276),\n",
       " ('内田真礼', 0.6406412124633789),\n",
       " ('矢作紗友里', 0.6399767994880676),\n",
       " ('大原さやか', 0.6346932053565979),\n",
       " ('金元寿子', 0.6332128047943115),\n",
       " ('喜多村英梨', 0.6319013237953186),\n",
       " ('佐藤利奈', 0.6254490613937378),\n",
       " ('後藤邑子', 0.6248806118965149)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = split_word(input(\"input text: \"),True).strip().split(\" \")\n",
    "fmodel.most_similar(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーラン: イスラム教の聖典  \n",
    "イスラム教: 宗教の一つ  \n",
    "旧約聖書: キリスト教の正典  \n",
    "イスラム教 - コーラン + 旧約聖書 = ????  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('キリスト教', 0.6492341756820679),\n",
       " ('ユダヤ教', 0.6367957592010498),\n",
       " ('キリスト教徒', 0.5953476428985596),\n",
       " ('新約聖書', 0.5876226425170898),\n",
       " ('マロン典礼カトリック教会', 0.5842365026473999),\n",
       " ('霊的キリスト教', 0.5791678428649902),\n",
       " ('聖書', 0.5702466368675232),\n",
       " ('キリスト', 0.5661367774009705),\n",
       " ('エチオピア正教会', 0.5649876594543457),\n",
       " ('ヘブライ語聖書', 0.5622918605804443)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel.most_similar(positive=[\"イスラム教\", \"旧約聖書\"], negative=[\"コーラン\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: かわいい\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('可愛い', 0.7294535636901855),\n",
       " ('かわいい女', 0.670622706413269),\n",
       " ('女の子', 0.6256632208824158),\n",
       " ('可愛らしく', 0.6034404635429382),\n",
       " ('可愛らしい', 0.6003689765930176),\n",
       " ('かわいく', 0.5996170043945312),\n",
       " ('かわいらしく', 0.5813488960266113),\n",
       " ('おしゃれ', 0.5782523155212402),\n",
       " ('大人っぽい', 0.5750479698181152),\n",
       " ('かわいらしい', 0.5676910281181335)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = split_word(input(\"input text: \"),True).strip().split(\" \")\n",
    "fmodel.most_similar(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext -pixiv小説モデルで遊んでみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "ffmodel = FastText.load_fasttext_format('MODEL/pixiv_novel/fasttext-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: かわいい\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('可愛い', 0.8984634876251221),\n",
       " ('Cawaii!', 0.846759021282196),\n",
       " ('ざとかわいい', 0.8390275239944458),\n",
       " ('エロかわいい', 0.7630234956741333),\n",
       " ('ぇかわいい', 0.7591103315353394),\n",
       " ('ずるい', 0.7465609908103943),\n",
       " ('カワイイー', 0.7461790442466736),\n",
       " ('ぁわいい', 0.7455922961235046),\n",
       " ('ーカワイイ', 0.744534969329834),\n",
       " ('かっこいい', 0.7317109704017639)]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = split_word(input(\"input text: \"),True).strip().split(\" \")\n",
    "ffmodel.most_similar(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: 佐倉綾音\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('伊藤かな恵', 0.8857293725013733),\n",
       " ('佐藤利奈', 0.8840634822845459),\n",
       " ('花澤香菜', 0.8804652094841003),\n",
       " ('水瀬いのり', 0.8723459839820862),\n",
       " ('今井麻美', 0.8675230741500854),\n",
       " ('早見沙織', 0.8672717213630676),\n",
       " ('岡本信彦', 0.866820216178894),\n",
       " ('日笠陽子', 0.8660044074058533),\n",
       " ('茅野愛衣', 0.8659030795097351),\n",
       " ('金元寿子', 0.8609911799430847)]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = split_word(input(\"input text: \"),True).strip().split(\" \")\n",
    "ffmodel.most_similar(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pmodel = gensim.models.KeyedVectors.load_word2vec_format('MODEL/pixiv_novel/fasttext-model.vec', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分かち書きをしなくて文字列を入力として候補を出力できる. 実験してみて似た雰囲気の言葉を返しているわけでもなさそう.. \n",
    "```python\n",
    "input_text = input('input text: ')\n",
    "print(\"-------------Not 分かち書き-------------\")\n",
    "for x in pmodel.most_similar(input_text):\n",
    "    print(x[0],x[1])\n",
    "print(\"--------------分かち書き(NE)----------------\")\n",
    "input_text0 = split_word(input_text,True).strip().split(\" \")\n",
    "print(input_text0)\n",
    "for x in pmodel.most_similar(positive=input_text0):\n",
    "    print(x[0],x[1])\n",
    "print(\"--------------分かち書き(default)----------------\")\n",
    "input_text1 = split_word(input_text,False).strip().split(\" \")\n",
    "print(input_text1)\n",
    "for x in pmodel.most_similar(input_text1):\n",
    "    print(x[0],x[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec-pixiv小説モデルで遊んでみる  \n",
    "doc2vecは辞書に含まれていない単語には対応できない(要出典)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## doc2vec でも遊ぶ\n",
    "from gensim.models import doc2vec\n",
    "pdmodel = doc2vec.Doc2Vec.load(\"MODEL/pixiv_novel/doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: 図書館戦争\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('クロスオーバー', 0.5370278358459473),\n",
       " ('二次創作', 0.5299734473228455),\n",
       " ('R指定', 0.5223528146743774),\n",
       " ('ネタバレ', 0.5196671485900879),\n",
       " ('設定', 0.5082077980041504),\n",
       " ('ノベライズ', 0.4985283613204956),\n",
       " ('スノーボード', 0.49830979108810425),\n",
       " ('ゲーム版', 0.49589353799819946),\n",
       " ('再掲', 0.4945579469203949),\n",
       " ('原作', 0.4923849403858185)]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = input(\"input text: \")\n",
    "pdmodel.most_similar(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: 図書館戦争\n",
      "split>> ['図書館', '戦争']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('幻想郷', 0.9157354235649109),\n",
       " ('学校', 0.9109996557235718),\n",
       " ('教会', 0.9004945158958435),\n",
       " ('聖域', 0.9004569053649902),\n",
       " ('江戸', 0.8955792188644409),\n",
       " ('地球', 0.8928031921386719),\n",
       " ('魔界', 0.8840538859367371),\n",
       " ('孤児院', 0.8830622434616089),\n",
       " ('天界', 0.8813377618789673),\n",
       " ('王都', 0.8804082870483398)]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = split_word(input(\"input text: \"),False).strip().split(\" \")\n",
    "print(\"split>>\",input_text)\n",
    "pdmodel.most_similar(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input >> 図書館\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('保健室', 0.9106063842773438),\n",
       " ('学校', 0.9057930707931519),\n",
       " ('王都', 0.905574381351471),\n",
       " ('道場', 0.9044901132583618),\n",
       " ('書庫', 0.9012769460678101),\n",
       " ('東京', 0.8968735337257385),\n",
       " ('事務所', 0.8929109573364258),\n",
       " ('城下', 0.8927860260009766),\n",
       " ('教会', 0.8926318287849426),\n",
       " ('食堂', 0.8922826647758484)]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"input >>\",input_text[0])\n",
    "pdmodel.most_similar(input_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 所感\n",
    "word2vecよりfasttextの方がほしい結果が出た気がする. ただ評価が難しい....  \n",
    "fasttextはword2vecやdoc2vecと異なり辞書に含まれていない単語に対しても何かしらの候補単語を示すことはできる.  ただし,それがどういう感なのかはいまいちわからない.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vecとfasttextの大きな違いはfasttextに組み込まれているsubword(部分語)情報を用いてる点である.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fmodel = gensim.models.KeyedVectors.load_word2vec_format('MODEL/model.vec', binary=False)\n",
    "def input_output(i,modelName):\n",
    "    '''\n",
    "    ffmodel: pixiv\n",
    "    fmodel :wikipeda\n",
    "    '''\n",
    "    #i = input(\"input text: \")\n",
    "    input_text = split_word(i,True).strip().split(\" \")\n",
    "    #print(input_text)\n",
    "    \n",
    "\n",
    "    #mecab-ipadic-neologd で分かち書きしてうまくいかなかった場合デフォルトの辞書で分かち書きして入力\n",
    "    try :\n",
    "        print(\"try: \",input_text)\n",
    "        for x in modelName.most_similar(input_text):\n",
    "            print(x)\n",
    "    except KeyError:\n",
    "        try:\n",
    "            input_text = split_word(i,False).strip().split(\" \")\n",
    "            print(\"except: \",input_text)\n",
    "            for x in modelName.most_similar(input_text):\n",
    "                print(x)\n",
    "        except KeyError:\n",
    "            try:\n",
    "                print(\"全部無理ぽ: \",i)\n",
    "                for x in modelName.most_similar([i]):\n",
    "                    print(x)\n",
    "            except:\n",
    "                print(\"----文字自体が辞書に含まれていませんでした----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try:  ['(*ﾟーﾟ)']\n",
      "(\"(*'\", 0.762630820274353)\n",
      "('(*>', 0.739618718624115)\n",
      "('＼*)', 0.738576352596283)\n",
      "(\"'*)\", 0.7343478202819824)\n",
      "('＾*)', 0.7328968048095703)\n",
      "('(*/∀＼*)', 0.729537844657898)\n",
      "('(*>∀<*)', 0.7287780046463013)\n",
      "('(*´∇`)', 0.7278110980987549)\n",
      "('(^▽^)', 0.7234911918640137)\n",
      "('(*‘ω‘', 0.7228749990463257)\n"
     ]
    }
   ],
   "source": [
    "input_output(\"(*ﾟーﾟ)\",ffmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try:  ['(*ﾟーﾟ)']\n",
      "except:  ['(*', 'ﾟーﾟ', ')']\n",
      "全部無理ぽ:  (*ﾟーﾟ)\n",
      "----文字自体が辞書に含まれていませんでした----\n"
     ]
    }
   ],
   "source": [
    "input_output(\"(*ﾟーﾟ)\",fmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try:  ['(´･ω･｀)']\n",
      "('バラ科', 0.374181866645813)\n",
      "('植え込む', 0.3652542233467102)\n",
      "('人為的', 0.35676005482673645)\n",
      "('摘み取る', 0.35282132029533386)\n",
      "('生存戦略', 0.3414807915687561)\n",
      "('生存権', 0.3391748368740082)\n",
      "('寄生植物', 0.3357234299182892)\n",
      "('宿り木', 0.3337258994579315)\n",
      "('幼体', 0.33261629939079285)\n",
      "('病原菌', 0.33160269260406494)\n"
     ]
    }
   ],
   "source": [
    "input_output(\"(´･ω･｀)\",ffmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try:  ['(´･ω･｀)']\n",
      "except:  ['(´･', 'ω', '･｀)']\n",
      "全部無理ぽ:  (´･ω･｀)\n",
      "----文字自体が辞書に含まれていませんでした----\n"
     ]
    }
   ],
   "source": [
    "input_output(\"(´･ω･｀)\",fmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try:  ['＼(^o^)／']\n",
      "('＼(^', 0.8958107233047485)\n",
      "('(^o^)/', 0.8782684803009033)\n",
      "('＼(^^)／', 0.8727320432662964)\n",
      "('o(^o^)o', 0.863396406173706)\n",
      "('^)／', 0.8564893007278442)\n",
      "('／(^o^)＼', 0.8384049534797668)\n",
      "('(￣｡￣;)', 0.8368974924087524)\n",
      "('＼(＾', 0.8368598222732544)\n",
      "('(^O^)／', 0.819638729095459)\n",
      "('(;O;)', 0.8176270127296448)\n"
     ]
    }
   ],
   "source": [
    "input_output(\"＼(^o^)／\",ffmodel)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "学習データに顔文字が含まれているpixivデータなら顔文字を検出できるが, wikipediaデータでは難しい"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
